#run args
seed: 420
env_id: Hopper-v4
device: cpu
total_timesteps: 1_000_000
run_path: ""
# logging args
log:
  wandb: False
  log_local: False
  save_models: False
  log_dir: logs
  model_dir: models
# sac args
sac:
  n_experts: 1
  q_depth: 2
  topk: 1
  policy_lr: 0.0003
  q_lr: 0.001
  buffer_size: 1_000_000
  gamma: 0.99  # discount factor
  tau: 0.005
  batch_size: 256
  policy_frequency: 2  # frequency of the policy training (delayed)
  target_network_frequency: 1
  alpha: 0.2  # temperature determining the relative importance of the entropy term against the reward
  alpha_auto: True
# decision-tree args
dt:
  n_ds_samples: 1_000_000  # number of samples to collect for the supervised tree-training
  max_depth: 2

