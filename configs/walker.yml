#run args
seed: 420
env_id: Walker2d-v4
device: cpu
total_timesteps: 1_000_000
run_path: ""
# logging args
log:
  wandb: False
  log_local: False
  save_models: False
# sac args
sac:
  n_experts: 6
  q_depth: 2
  topk: 1
  policy_lr: 0.0003
  router_hidden_dims: []
  q_lr: 0.001
  buffer_size: 1_000_000
  gamma: 0.99  # discount factor
  tau: 0.005
  batch_size: 256
  policy_frequency: 2  # frequency of the policy training (delayed)
  target_network_frequency: 1
  alpha: 0.2  # temperature determining the relative importance of the entropy term against the reward
  alpha_auto: True
  prune_start: 0.2
  prune_end: 0.8
  prune_percent: 0.9

